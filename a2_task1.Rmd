---
title: "Assignment 2 Task 1 - Binary Logistic Regression (individual)"
author: "Matthieu Huy"
date: "2023-02-08"
output: html_document
---

```{r setup, echo = TRUE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(here)
library(cowplot)
library(GGally)
library(AICcmodavg)
library(jtools)

```

The data used in this analysis, collected at Archbold Biological Station in south-central Florida from 1981 to 2017, measures the survival, growth, and biomass of two dominant palmetto tree species, Serenoa repens and Sabal etonia, collected at Archbold Biological Station in south-central Florida from 1981 to 2017. This data also records the height, canopy length and width, number of new and green leaves, and flowering scapes.\

Citation:\
- Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\
(Accessed 2023-02-08).\

Metadata:\
- https://portal.edirepository.org/nis/metadataviewer?packageid=edi.317.1


For this analysis, I will compare two binary logistic regression models using 10-fold cross-validation to determine the probability of a plant being of either species based on several predictor variables.\

- Model 1: Log odds of plant type using plant height, canopy length, canopy width, and green leaves as predictor variables.\
- Model 2: Log odds of plant type using plant height, canopy width, and green leaves as predictor variables (dropping canopy length).\

### Read in and clean data

```{r}
palmetto_data <- read_csv(here("data/palmetto.csv")) |> 
  select(species, survival, height, canopy_length = length, canopy_width = width, green_lvs) |>
  mutate_at(vars(1:2), as.factor) |> #change species and survival columns to factor levels
  filter(survival == 1) |> #select only the trees marked "alive", removing dead and missing trees
  select(-survival) |>
  mutate_at(vars(2:5), as.numeric) #change height, length, width, and leaves columns to numeric

levels(palmetto_data$species) <- c("Serenoa repens", "Sabal etonia")
#shows species "1" - Serenoa repens - is the reference level 0
```

### Data visualization

```{r}
GGally::ggpairs(palmetto_data |> #takes all variables in data set and generates grid of plots
                select(species, height:green_lvs),
                aes(color = species))
```

```{r}

plot1 <- ggplot() + 
  geom_density(data = palmetto_data,
               aes(x = green_lvs,
                   fill = species,
                   color = species),
               alpha = 0.6) +
  theme_minimal() +
  labs(x = "Green leaves",
       y = "Percent of observations",
       fill = "Species",
       color = "Species") +
  theme(axis.text = element_text(color = "black", size = 6),                    #fonts/font sizes
        axis.title.x = element_text(color = "black", size = 10), 
        axis.title.y = element_text(color = "black", size = 10))

legend <- get_legend(plot1) #extract legend

plot1 <- plot1 +
  theme(legend.position = "none")

plot2 <- ggplot() +
  geom_point(data = palmetto_data,
             aes(x = height,
                 y = canopy_length,
                 color = species),
             alpha = 0.6) +
  theme_minimal() +
  labs(x = "Height (cm)",
       y = "Canopy Length (cm)",
       color = "Species") +
  theme(axis.text = element_text(color = "black", size = 6),                    #fonts/font sizes
        axis.title.x = element_text(color = "black", size = 10), 
        axis.title.y = element_text(color = "black", size = 10),
        legend.position = "none")

plot3 <- ggplot() +
  geom_point(data = palmetto_data,
             aes(x = canopy_length,
                 y = canopy_width,
                 color = species),
             alpha = 0.6) +
  theme_minimal() +
  labs(x = "Canopy Length (cm)",
       y = "Canopy Width (cm)",
       color = "Species") +
  theme(axis.text = element_text(color = "black", size = 6),                    #fonts/font sizes
        axis.title.x = element_text(color = "black", size = 10), 
        axis.title.y = element_text(color = "black", size = 10),
        legend.position = "none")

#create title
title <- ggdraw() + 
  draw_label(
    "Visualizing differences in green leaves, height, canopy length, and canopy width between species",
    fontface = 'bold',
    size = 10,
    x = 0,
    hjust = 0
  ) +
  theme(
    # add margin on the left of the drawing canvas,
    # so title is aligned with left edge of first plot
    plot.margin = margin(0, 0, 0, 7)
  )


plots <- plot_grid(plot1, plot2, plot3, legend)

plot_grid(title, plots,
          ncol = 1,
  # rel_heights values control vertical title margins
          rel_heights = c(0.1, 1))

  
  
```

As shown in the top-left plot, there appears to be a significant difference in green leaves between the two species, with most Sabal etonia trees concentrated in the <5 range, and Serenoa repens trees more broadly spread but mostly in the >5 range. Both species seem to have a similar distribution of heights and canopy widths, and there appears to be small differences canopy length, with more observations of Sabal etonia with large canopy lengths. Based on these plots, green leaves and canopy length are likely to serve as the best predictor variables to help classify species.\

### Binary Logistic Regression\

Compare models to determine the probability of a plant being either Serenoa repens or Sabal etonia based on several predictor variables.

```{r, results='hide'}
#create model 1 and model 2

f1 <- species ~ height + canopy_length + canopy_width + green_lvs #create formula for model 1

f2 <- species ~ height + canopy_width + green_lvs #create formula for model 2

palm_predict_blr1 <- glm(formula = f1, data = palmetto_data, #create blr model 1
                         family = "binomial")  #specifies we want binary logistic regression

palm_predict_blr2 <- glm(formula = f2, data = palmetto_data, #create blr model 2
                         family = "binomial")  

summary(palm_predict_blr1)
summary(palm_predict_blr2)
AICcmodavg::aictab(list(palm_predict_blr1, palm_predict_blr2))
#delta AIC = 792, overwhelmingly in favor of model 1
AICcmodavg::bictab(list(palm_predict_blr1, palm_predict_blr2))
#delta BIC = 785, overwhelmingly in favor of model 1
```

#### Observations:\

- Notice that in model 1, all of the variables are significant.
- Removing canopy length in model 2 significantly increases the p-value of height, making height no longer significant. Does this suggest collinearity between height and canopy length? Our initial data visualization suggests this may be the case.\
- delta AIC = 792, overwhelmingly in favor of model 1\
- delta BIC = 785, overwhelmingly in favor of model 1\

### 10-fold cross validation\

#### using purrr::map() and predict() functions

```{r}
set.seed(123) #arbitrary number, tells to select the same set of random numbers every time its run

n_folds <- 10 #number of folds

fold_vec <- rep(1:n_folds, length.out = nrow(palmetto_data)) #create folds vector

palm_kfold <- palmetto_data |> 
  mutate(fold = sample(fold_vec, #add column assigning fold # to each value
                       size = n(),
                       replace = FALSE))
```

```{r}
# function to calculate accuracy, given a "truth" vector and "prediction" vector
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  return(mean(accurate, na.rm = TRUE))
}

calc_fold <- function(i, fold_df, f) { #fold number, dataframe with folds, formula for blr
  kfold_test <- fold_df |> #create test data
    filter(fold == i) #select one fold to test on
  kfold_train <- fold_df |> #create training data
    filter(fold != i) #select other remaining 9 folds as training set
  
  kfold_blr <- glm(f, data = kfold_train, family = "binomial") #create model using train
  #use model to predict on test data
  kfold_pred <- kfold_test |> #calculate how accurate the predictions from model work
    mutate(blr = predict(kfold_blr, kfold_test, type = "response"),
           pred = ifelse(blr > 0.50, "Sabal etonia", "Serenoa repens"))
  #set probability threshold for classification
  kfold_accuracy <- kfold_pred |> 
    summarize(blr_acc = pred_acc(species, pred))
  
  return(kfold_accuracy)
}

results1_purrr_df <- purrr::map(.x = 1:n_folds, # sequence of fold numbers
                                .f = calc_fold, # function
                                fold_df = palm_kfold, # additional argument to calc_fold()
                                f = f1) |>              # additional argument to calc_fold()
  bind_rows() |>
  mutate(mdl = 'f1')

results2_purrr_df <- purrr::map(.x = 1:n_folds, .f = calc_fold, 
                               fold_df = palm_kfold,
                               f = f2) |>
  bind_rows() |>
  mutate(mdl = 'f2')

results_purrr_df <- bind_rows(results1_purrr_df, results2_purrr_df) |>
  group_by(mdl) |>
  summarize(mean_acc = mean(blr_acc))

results_purrr_df
```

#### cross validation using tidymodels 

```{r}
### set seed for reproducibility! here to set the folds
set.seed(123)

tidy_folds <- vfold_cv(palmetto_data, v = 10) #assign folds to adelie_chinstrap

### use a workflow that bundles the logistic model and a formula

blr_model <- logistic_reg() |>
  set_engine('glm')

blr_tidy_wf1 <- workflow() |> #sets up a "skeleton" code to be able to change components
  add_model(blr_model) |>
  add_formula(f1)

blr_tidy_cv_f1 <- blr_tidy_wf1 |> 
  fit_resamples(tidy_folds) #apply workflow to tidy_folds data frame

# use functions from the tune package to extract metrics
collect_metrics(blr_tidy_cv_f1)

#repeat for model 2
blr_tidy_wf2 <- workflow() |>
  add_model(blr_model) |>
  add_formula(f2)
blr_tidy_cv_f2 <- blr_tidy_wf2 |>
  fit_resamples(tidy_folds)

collect_metrics(blr_tidy_cv_f2)
```

Performing a 10-fold cross validation using the purrr::map() and tidy models both confirm that model 1 has a higher accuracy when predicting species, although both models perform well. Model 1 has a mean accuracy of 91.7% compared to 89.9 for model 2.

### Parameterize final model using the whole data set

AIC/BIC comparison and cross validation both indicate model 1 as the best model, so we will use it as our final model.

```{r}
final_mdl <- glm(formula = f2, data = palmetto_data,
                         family = "binomial") 

#Final model regression results table
finalmdl_tidy <- tidy(final_mdl) |>
  kable(caption = "Final Model") |> 
  kable_classic()

finalmdl_tidy
```
\

```{r}
blr1_fitted <- final_mdl |> 
  broom::augment(type.predict = "response") |>  #turns log-odds into probability value 
  mutate(species_pred = ifelse(.fitted >= 0.50, "Sabal etonia", "Serenoa repens")) |>
  mutate(match = ifelse(species_pred == species, "TRUE", "FALSE")) 

correctly_classified = sum(blr1_fitted$match == "TRUE", na.rm = TRUE)
falsely_classified = sum(blr1_fitted$match == "FALSE", na.rm = TRUE)

final_table <- data.frame(
  correctly_classified = sum(blr1_fitted$match == "TRUE", na.rm = TRUE),
  falsely_classified = sum(blr1_fitted$match == "FALSE", na.rm = TRUE)) |>
  mutate(pct_correct = 
           100*(correctly_classified/(correctly_classified+falsely_classified))) |>
  rename("# of Trees Correctly Classified" = correctly_classified,
         "# of Trees Incorrectly Classified" = falsely_classified,
         "% Correctly Classified" = pct_correct) |>
  kable(caption = "Performance of Final Model") |>
  kable_classic()

final_table
```
\
\
